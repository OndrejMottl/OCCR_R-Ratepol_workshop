---
title: Step-by-step guide
format: 
  gfm:
    fig-width: 7
    fig-height: 6
    wrap: none

---

```{r chunk-setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This workflow should show the full strength of the [*RRatepol package*](https://hope-uib-bio.github.io/R-Ratepol-package/) for working with data types **other** than fossil polle. It should serve as step-by-step guidance starting from downloading dataset from Neotoma, building age-depth models, to estimating rate-of-change using age uncertainty.

‚ö†Ô∏è**This workflow is only meant as an example**: There may be several additional steps for data preparation which should be done to properly implement RRatepol and asssess rate of change for your specific project. 

For a step-by-spet workflow with fossil pollen data, please see other package materials, such as [African Polled Database workshop](https://ondrejmottl.github.io/APD_R-Ratepol_workshop/index.html).

Additionally, please see [**FOSSILPOL**](https://hope-uib-bio.github.io/FOSSILPOL-website/), an R-based modular workflow to process multiple fossil pollen records to create a comprehensive, standardised dataset compilation, ready for multi-record and multi-proxy analyses at various spatial and temporal scales.

## Install packages

Please follow the [pre-workshop instructions](https://ondrejmottl.github.io/OCCR_R-Ratepol_workshop/pre_workshop.html) to make sure all packages are installed.

## Attach packages

```{r pkg-attach, results='hide', warning=FALSE, message=FALSE}
library(tidyverse) # general data wrangling and visualisation ‚ú®
library(pander) # nice tables üòç
library(RRatepol) # rate-of-vegetation change ! > v1.2.0 ! üìà
library(neotoma2) # access to the Neotoma database üåø
library(Bchron) # age-depth modelingng üï∞Ô∏è
library(janitor) # string cleaning üßπ
library(here) # for working directory üó∫Ô∏è
```

```{r theme-setup, include=FALSE}
ggplot2::theme_set(
  ggplot2::theme_bw() +
    ggplot2::theme(
      axis.title = ggplot2::element_text(size = 25),
      axis.text = ggplot2::element_text(size = 15),
      strip.text = ggplot2::element_text(size = 15),
      panel.grid = ggplot2::element_blank()
    )
)
```

## Download a dataset from Neotoma

Here we have selected the **Chickaree Lake** record (ID = 47613) by Higuera, Philip E. and Dunnette, Paul V.

Reference paper: Dunnette, P.V., P.E. Higuera, K.K. McLauchlan, K.M. Derr, C.E. Briles, and M.H. Keefe. 2014. Biogeochemical impacts of wildfires over four millennia in a Rocky Mountain subalpine watershed. New Phytologist 203(3):900-912. [DOI: 10.1111/nph.12828]

```{r download_of_data, results='hide', warning=FALSE, message=FALSE}
sel_dataset_download <-
  neotoma2::get_downloads(47613)
```

## Prepare the geochemical data

```{r geochemistry_preparation, results='hide', warning=FALSE}
# get samples
data_samples <-
  neotoma2::samples(sel_dataset_download)  %>% 
  tibble::as_tibble()

# prepare taxa table
data_community <-
  data_samples %>%
  dplyr::mutate(sample_id = as.character(sampleid)) %>%
  dplyr::select("sample_id", "value", "variablename") %>%
  # turn into the wider format
  tidyr::pivot_wider(
    names_from = "variablename",
    values_from = "value",
    values_fill = 0
  ) %>%
  # clean names
  janitor::clean_names()  %>% 
  # remove the ratio as it is just calcualtion from other variables
  dplyr::select(-carbon_nitrogen)

# make table with units for later
data_units <-
  data_samples %>%
  dplyr::distinct(variablename, units)  %>% 
  dplyr::filter(
    variablename != "Carbon:Nitrogen"
  )

head(data_community)[, 1:5]
```

```{r geochemistry_diplay, echo=FALSE, results='asis', warning=FALSE, message=FALSE}
pander::pandoc.table(head(data_community)[, 1:5])
```

Here, we strongly advocate that careful preparation may of the datasets (with additional steps) may be needed before using R-Ratepol!

We can now try to visualise the taxa per sample_id

```{r geochemistry_vis}
data_community %>%
  tibble::rowid_to_column("ID") %>%
  tidyr::pivot_longer(
    cols = -c(sample_id, ID),
    names_to = "element",
    values_to = "value"
  ) %>%
  ggplot2::ggplot(
    mapping = ggplot2::aes(
      x = ID,
      y = value,
      col = element
    ),
  ) +
  ggplot2::facet_wrap(
    ~ element,
    nrow = 1,
    scales = "free_x"
  ) +
  ggplot2::coord_flip() +
  ggplot2::theme(
    panel.grid.major.x = ggplot2::element_line(
      colour = "grey",
      linewidth = 0.1
    ),
    axis.text.y = ggplot2::element_blank(),
    axis.ticks.y = ggplot2::element_blank(), 
    legend.position = "none"
  ) +
  ggplot2::labs(
    y = "sample id",
    x = "value"
  )  +
  ggplot2::geom_line()  
```

## Preparation of the levels

### Sample depth

Extract depth for each level

```{r level_preparion, results='hide', warning=FALSE}
data_levels <-
  neotoma2::samples(sel_dataset_download) %>%
  tibble::as_tibble() %>%
  dplyr::mutate(sample_id = as.character(sampleid)) %>%
  dplyr::distinct(sample_id, depth) %>%
  dplyr::relocate(sample_id)

head(data_levels)
```

```{r level_diplay, echo=FALSE, results='asis', warning=FALSE, message=FALSE}
pander::pandoc.table(head(data_levels))
```

### Age-depth modelling

We will recalculate the age-depth model 'de novo' using the [*Bchron* package](http://andrewcparnell.github.io/Bchron/). 

#### Prepare chron.control table and run Bchron
The chronology control table contains all the dates (mostly radiocarbon) to create the age-depth model.

Here we only present a few of the important steps of preparation of the chronology control table. There are many more potential issues, but solving those is not the focus of this workflow.

```{r chron_prepare, results='hide'}
# First, get the chronologies and check which we want to use used
sel_chron_control_table_download <-
  neotoma2::chroncontrols(sel_dataset_download)

print(sel_chron_control_table_download)
```
```{r chron_prepare_show, echo=FALSE, results='asis', warning=FALSE, message=FALSE}
pander::pandoc.table(head(sel_chron_control_table_download))
```

```{r chron_control_prepare, results='hide', warning=FALSE}
# prepare the table
data_chron_control_table <-
  sel_chron_control_table_download %>%
  # Here select the ID of one of the chronology
  dplyr::filter(chronologyid == 33168) %>%
  tibble::as_tibble() %>%
  # Here we calculate the error as the average of the age `limitolder` and
  #   `agelimityounger`
  dplyr::mutate(
    error = round((agelimitolder - agelimityounger) / 2)
  ) %>%
  # As Bchron cannot accept a error of 0, we need to replace the value with 1
  dplyr::mutate(
    error = replace(error, error == 0, 1),
    error = ifelse(is.na(error), 1, error)
  ) %>%
  # We need to specify which calibration curve should be used for what point
  dplyr::mutate(
    curve = ifelse(as.data.frame(sel_dataset_download)["lat"] > 0, "intcal20", "shcal20"),
    curve = ifelse(chroncontroltype != "Radiocarbon", "normal", curve)
  ) %>%
  tibble::column_to_rownames("chroncontrolid") %>%
  dplyr::arrange(depth) %>%
  dplyr::select(
    chroncontrolage, error, depth, thickness, chroncontroltype, curve
  )

head(data_chron_control_table)
```

```{r chron_control_show, echo=FALSE, results='asis', warning=FALSE, message=FALSE}
pander::pandoc.table(head(data_chron_control_table))
```

As this is just a toy example, we will use only the iteration multiplier (`i_multiplier`) of `0.1` to reduce the computation time. However, we strongly recommend increasing it to 5 for any normal age-depth model construction.
```{r bchron, results='hide', warning=FALSE, message=FALSE}
i_multiplier <- 0.1 # increase to 5

# Those are default values suggested by the Bchron package
n_iteration_default <- 10e3
n_burn_default <- 2e3
n_thin_default <- 8

# Let's multiply them by our i_multiplier
n_iteration <- n_iteration_default * i_multiplier
n_burn <- n_burn_default * i_multiplier
n_thin <- max(c(1, n_thin_default * i_multiplier))

# run Bchron
sel_bchron <-
  Bchron::Bchronology(
    ages = data_chron_control_table$chroncontrolage,
    ageSds = data_chron_control_table$error,
    positions = data_chron_control_table$depth,
    calCurves = data_chron_control_table$curve,
    positionThicknesses = data_chron_control_table$thickness,
    iterations = n_iteration,
    burn = n_burn,
    thin = n_thin
  )
```

Visually check the age-depth models

```{r bchron_figure, results='markup', warning=FALSE}
plot(sel_bchron)
```

#### Predict ages

Let's first extract posterior ages (i.e. possible ages) from the age-depth model.  

```{r age_uncertainties, results='hide', warning=FALSE}
age_position <-
  Bchron:::predict.BchronologyRun(object = sel_bchron, newPositions = data_levels$depth)

age_uncertainties <-
  age_position %>%
  as.data.frame() %>%
  dplyr::mutate_all(., as.integer) %>%
  as.matrix()

colnames(age_uncertainties) <- data_levels$sample_id

head(age_uncertainties, n = 8)[, 1:8]
```

Here we see samples (e.g., 439811, 439812, 439813,...) and their possible ages (age-sequence) with each model iteration (posterior). Each age-sequence is similar but there are differences of tens or hundreds of years. We will call this *the uncertainty matrix*.

```{r age_uncertainties_display, echo=FALSE, results='asis', warning=FALSE, message=FALSE}
pander::pandoc.table(head(age_uncertainties, n = 8)[, 1:8])
```
We can visualise these "possible ages" (age-sequence) of each iteration.

```{r age_uncertainties_vis_data}
# create a data.frame for plotting
data_age_uncertainties <-
  age_uncertainties %>%
  as.data.frame() %>%
  tibble::rowid_to_column("ID") %>%
  tidyr::pivot_longer(
    cols = -ID,
    names_to = "sample_id",
    values_to = "age"
  ) %>%
  dplyr::left_join(
    data_levels,
    by = dplyr::join_by(sample_id)
  )
```

Each line is a single potential age-depth model iteration (age-sequence). Green points represent the radiocarbon dates. Horizontal lines are depths of our samples.

```{r age_uncertainties_vis_lines}
(
  fig_age_uncertainties <-
    data_age_uncertainties %>%
    ggplot2::ggplot(
      mapping = ggplot2::aes(
        x = age,
        y = depth
      )
    ) +
    ggplot2::geom_line(
      mapping = ggplot2::aes(
        group = ID
      ),
      alpha = 0.05,
      linewidth = 0.1
    ) +
    ggplot2::geom_hline(
      yintercept = data_levels$depth,
      lty = 2,
      color = "gray50",
      alpha = 0.5,
      linewidth = 0.1
    ) +
    ggplot2::geom_point(
      data = data_chron_control_table,
      mapping = ggplot2::aes(
        x = chroncontrolage
      ),
      color = "green",
      shape = 15,
      size = 3
    ) +
    ggplot2::scale_y_continuous(trans = "reverse") +
    ggplot2::scale_x_continuous(trans = "reverse")
)
```

We can visualise all age-depth "possible ages" together as the range of values. Here, each line representing one sampled depth in our record.

```{r age_uncertainties_vis_boxplot}
data_age_uncertainties %>%
  ggplot2::ggplot(
    mapping = ggplot2::aes(
      x = age,
      y = depth,
      group = depth
    )
  ) +
  ggplot2::geom_hline(
    yintercept = data_levels$depth,
    lty = 2,
    color = "gray50",
    alpha = 0.5,
    linewidth = 0.1
  ) +
  ggplot2::geom_boxplot(
    outlier.shape = NA
  )
```

Let's take the median age of all possible ages (i.e. the estimated age from each age-depth model run) as our default.

```{r age_uncertainties_vis_table, results='hide', warning=FALSE}
data_levels_predicted <-
  data_levels %>%
  dplyr::mutate(
    age = apply(
      age_uncertainties, 2,
      stats::quantile,
      probs = 0.5
    )
  )

head(data_levels_predicted)
```

```{r age_uncertainties_vis_table_display, echo=FALSE, results='asis', warning=FALSE, message=FALSE}
pander::pandoc.table(head(data_levels_predicted))
```

We can visualise the median age by drawing a red line. This age is the age that is often reported in publications but in essence it represents multiple age-depth model runs with smaller or larger age uncertainties throughout the record.

```{r age_uncertainties_vis_median}
fig_age_uncertainties +
  ggplot2::geom_point(
    data = data_levels_predicted,
    color = "red",
    size = 3
  ) +
  ggplot2::geom_line(
    data = data_levels_predicted,
    color = "red",
    linewidth = 1
  )
```

### Visualisation of our data

Let's now make a simple pollen diagram with proportions of the main pollen taxa (x-axis) against our estimated ages along depth (y-axis).

```{r vis_data_with_ages}
data_community %>%
  dplyr::inner_join(
    data_levels_predicted,
    by = dplyr::join_by(sample_id)
  ) %>%
  tidyr::pivot_longer(
    cols = -c(sample_id, depth, age),
    names_to = "element",
    values_to = "value"
  ) %>%
  ggplot2::ggplot(
    mapping = ggplot2::aes(
      x = age,
      y = value,
      col = element
    ),
  ) +
  ggplot2::facet_wrap(
    ~element,
    nrow = 1,
    scales = "free_x"
  ) +
  ggplot2::coord_flip() +
  ggplot2::scale_x_continuous(trans = "reverse") +
  ggplot2::theme(
    panel.grid.major.x = ggplot2::element_line(
      colour = "grey",
      linewidth = 0.1
    ),
    legend.position = "none"
  ) +
  ggplot2::labs(
    x = "age (cal yr BP)",
    y = "value"
  ) +
  ggplot2::geom_line()
```


## Estimation Rate-of-Change

Now we will use our prepared geochemistry data and age-depth model to estimate the rate of change.
We will present several scenarios (i.e. approaches) to calculate RoC. 

### Selection of dissimilarity coefficient 

We can check the units of individual measured values:
```{r data_units results='hide', warning=FALSE}
data_units
```

```{r data_units_display, echo=FALSE, results='asis', warning=FALSE, message=FALSE}
pander::pandoc.table(data_units)
```

As we can see, all measured values are in difrent units. Therefore,for all scenarios, we will be using the `gower` dissimilarity coefficient (works with data in various units), and `time_standardisation` == 500 (this means that all ROC values are 'change per 500 yr').

### Scenario 1 - Estimating RoC for each level

This is the "Classic" approach that uses each sampled depth in a record (i.e. individual level) to estimate RoC.

```{r roc_sc1, results='hide', warning=FALSE, message=FALSE}
scenario_1 <-
  RRatepol::estimate_roc(
    data_source_community = data_community,
    data_source_age = data_levels_predicted,
    dissimilarity_coefficient = "gower",
    time_standardisation = 500,
    working_units = "levels" # here is set to use individual levels
  )
```

```{r roc_sc1_vis, results='markup', echo=TRUE}
RRatepol::plot_roc(data_source = scenario_1)
```

### Scenario 2 - Estimating RoC for each level with smoothing of data

We do the same as in Scenario 1 but now we smooth the community data before calculating RoC. This may be usefull to mitigate the error of measumerments. Specifically, we will add `smooth_method` = "shep" (i.e. Shepard's 5-term filter).

```{r roc_sc2, results='hide', warning=FALSE, message=FALSE}
scenario_2 <-
  RRatepol::estimate_roc(
    data_source_community = data_community,
    data_source_age = data_levels_predicted,
    dissimilarity_coefficient = "gower",
    time_standardisation = 500,
    working_units = "levels",
    smooth_method = "shep" # Shepard's 5-term filter
  )
```

```{r roc_sc2_vis, results='markup', echo=TRUE}
RRatepol::plot_roc(data_source = scenario_2)
```

We see that the absolute RoC scores are similar ut the pattern changed slightly (x-axis). 

### Scenario 3 - Estimating RoC for each level and calculating age uncertainties

For RoC analysis, it is important to consider age uncertainties. For each iteration, RRatepol will randomly select one age-sequence from the uncertainty matrix (see the age-depth modelling section for more info). 

```{r roc_sc3_rand, results='hide', warning=FALSE, message=FALSE}
set_randomisations <- 1000
```

In order to do that we need to increase the number of randomisations. This is again a toy example for a quick computation and therefore we only do 100 randomisations. We would recommend increasing the *set_randomisations* to 10.000 for any real estimation. 
To speed the process up, you can also set `use_parallel` == `TRUE`, which will use all cores of your computer.

```{r roc_sc3, results='hide', warning=FALSE, message=FALSE}
scenario_3 <-
  RRatepol::estimate_roc(
    data_source_community = data_community,
    data_source_age = data_levels_predicted,
    dissimilarity_coefficient = "gower",
    working_units = "levels",
    time_standardisation = 500,
    smooth_method = "shep",
    rand = set_randomisations, # set number of randomisations
    use_parallel = TRUE, # do use parallel computing
    age_uncertainty = age_uncertainties # Add the uncertainty matrix
  )

View(scenario_3)

summary(scenario_3)
```

We will now also visualize uncertainty around the RoC scores shown by a grey shadow.

```{r roc_sc3_vis, results='markup', echo=TRUE}
RRatepol::plot_roc(data_source = scenario_3)
```

Here you can see that the uncertainty (grey shadow) around the RoC scores (black line) increased drastically. This is because we are randomly sampling age and taxa with a small number of randomisations.

### Scenario 5 - Estimating RoC per bin

In order to get rid of the effect of uneven distribution of sampled depths (i.e. levels) in a fossil pollen record, we can bin the data.
Specifically, we will change the `working_units` from single levels to `"bins"`. Here we select bins of 500 years each instead of the individual levels. Note that one level is randomly selected as a representation of that time bin.

```{r roc_sc5, results='hide', warning=FALSE, message=FALSE}
scenario_5 <-
  RRatepol::estimate_roc(
    data_source_community = data_community,
    data_source_age = data_levels_predicted,
    dissimilarity_coefficient = "gower",
    working_units = "bins", # change the "bins"
    bin_size = 500, # sie of a time bin
    time_standardisation = 500,
    smooth_method = "shep",
    standardise = TRUE,
    n_individuals = 150,
    rand = set_randomisations,
    use_parallel = TRUE,
    age_uncertainty = age_uncertainties
  )
```

```{r roc_sc5_vis, results='markup', echo=TRUE}
RRatepol::plot_roc(data_source = scenario_5)
```

We see a substantial increase in temporal uncertainty around the RoC scores (grey shadow), indicating a loss of temporal precision.

### Scenario 6 A - Estimating RoC with the new "Moving-window" approach

In order to reduce the temporal uncertainty and improve temporal precision, we can apply a novel approach in RRATEPOL called "moving window". 

```{r roc_sc6, results='hide', warning=FALSE, message=FALSE}
scenario_6 <-
  RRatepol::estimate_roc(
    data_source_community = data_community,
    data_source_age = data_levels_predicted,
    dissimilarity_coefficient = "gower",
    working_units = "MW", # change the "MW" to apply the "moving window"
    bin_size = 500,
    number_of_shifts = 5, # number of shifts
    time_standardisation = 500,
    smooth_method = "shep",
    standardise = TRUE,
    n_individuals = 150,
    rand = set_randomisations,
    use_parallel = TRUE,
    age_uncertainty = age_uncertainties
  )
```

```{r roc_sc6_vis, results='markup', echo=TRUE}
RRatepol::plot_roc(data_source = scenario_6)
```

### Scenario 6 B - Estimating RoC with the new "Moving-window" approach and detecting peak points

Throughout the record, there can be periods when the RoC will substantially change. We can detect RoC increases that are significant by identifying so called  *peak-points*. Here, we will use "Non-linear" method, which will detect significant change from a non-linear trend of RoC.

```{r peak_points, results='hide', warning=FALSE}
scenario_6_peak <-
  RRatepol::detect_peak_points(
    data_source = scenario_6,
    sel_method = "trend_non_linear"
  )
```

Now we will plot the RoC estimates showing the peak-points. So here we can see that there were rates of vegetation change throughout the record but only at certain moments in time (green dots - peak points) these changes were significant. There you go!

```{r peak_points_vis, results='markup', echo=TRUE}
RRatepol::plot_roc(
  data_source = scenario_6_peak,
  peaks = TRUE
)
```